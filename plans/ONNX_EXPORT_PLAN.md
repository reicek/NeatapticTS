# ONNX Export / Import Plan for NeatapticTS

_Last updated: 2025-08-17 (Phase 4 progressing: Conv2D + Pool + sharing validation + heuristic conv inference metadata + flatten-after-pool option; Phase 3 Step 6 deep tests still pending)_

## 0. Purpose
Provide a clear, incremental roadmap from the current minimal MLP ONNX export/import toward broad ONNX compatibility (feed‑forward, recurrent, convolutional, attention, quantized, and extensible custom ops) while preserving NeatapticTS evolutionary features where feasible.

## 1. Current Status (Implemented)
File: `src/architecture/network/network.onnx.ts`

Completed (Phase 0):
- Export (`exportToONNX`) of strictly layered, fully-connected feed‑forward MLPs (inputs → 0+ hidden layers → outputs).
- Import (`importFromONNX`) for models generated by our exporter (round‑trip reproducibility).
- Layer inference & validation: detects non-layered or missing full connectivity; enforces homogeneous activation per non-input layer.
- Supported activations mapped to ONNX: ReLU, Tanh, Sigmoid (Logistic), Identity (fallback with warning for unknown custom activations).
- Parameter export: weights + biases as ONNX initializers (float32 scalars in `float_data`).
- Graph structure assembly with a pair of nodes per layer transition (Gemm + Activation) — currently activation node is emitted before Gemm (historical project ordering, but non-standard for ONNX).
- Bias handling integrated in Gemm (alpha=1, beta=1, transB=1 attributes included).
- Single-layer perceptron edge case handled (inputs → outputs only).

## 2. Gaps vs. Initial Intent
Original plan suggested a per-node mapping (Add / MatMul explicit). Implementation instead uses Gemm (fused MatMul + Add) + Activation pairs — more compact but diverges from the bullet description. Update accepted as design choice; doc corrected accordingly.

## 3. Known Limitations (Current)
- No model metadata (IR version, opset_import, producer_name, doc_string) — output is a lightweight partial ONNX-like JSON, not a spec-complete protobuf structure.
- Node ordering is atypical (Activation before Gemm) — may confuse external tooling performing simple sequential execution assumptions.
- Only float32 dense MLPs; no batching dimension (shape is 1D feature vector, no dynamic axes).
- Homogeneous activation constraint per layer (cannot mix activations inside a layer; output layer forced homogeneous too).
- No support for: sparsity, dropout, batch norm, residual/skip connections, concatenations, branching graphs, shared weights (tying), or partial connectivity.
- Recurrent architectures (Elman/Jordan, LSTM/GRU, arbitrary recurrent links) unsupported.
- Convolutional, pooling, normalization, attention/transformer ops unsupported.
- No quantization, mixed precision, or half/INT8 export.
- Gating & NEAT-specific structural innovations ignored (cannot serialize gating semantics into ONNX yet).
- Custom activations degrade silently to Identity (warning only); no FunctionProto / custom domain registration.
- Import path assumes naming convention `W{idx}`, `B{idx}`, activation node count == layer count; fragile if altered.
- No graph-level validation suite or ONNX Runtime compatibility tests; only internal structural guarantees.

## 4. Design Principles Going Forward
1. Determinism: deterministic ordering of initializers and nodes to allow hashing/comparisons.
2. Spec compliance: progress from a lightweight JSON to either (a) fully spec-compliant JSON matching ONNX ModelProto schema or (b) direct protobuf encoding via an optional dependency.
3. Extensibility: pluggable activation & composite op registry mapping internal functions to ONNX standard ops or custom domains.
4. Graceful degradation: always export a valid subgraph even when certain advanced features need approximation (e.g., replace unsupported activation with Identity + metadata note).
5. Evolution fidelity: preserve NEAT-relevant metadata (innovation numbers, topology tags) in model metadata without breaking external consumers.

## 5. Phased Roadmap

### Phase 1 (Immediate Hardening) - COMPLETED
Implemented:
- Standard node ordering Gemm → Activation (default) with legacy ordering flag.
- `OnnxExportOptions` including metadata, batch dimension, legacy ordering.
- Metadata fields: `ir_version`, `opset_import`, `producer_name`, `producer_version`, `doc_string` (optional via flag).
- Optional batch dimension with symbolic `N`.
- Round-trip numerical equivalence tests (`onnx.roundtrip.test.ts`).
- Layer-level heterogeneous output activation difference (output layer activation can differ from hidden layers) already supported by per-layer homogeneity rule.

### Phase 2 (Feature Breadth – Feedforward Enhancements) - COMPLETED
Deliverables achieved:
1. Partial connectivity export (validation relaxation + zero-weight insertion).
2. Mixed per-neuron activations (decomposition to Gemm+Activation per neuron + Concat) with reversible import.
3. Metadata enrichment (`metadata_props.layer_sizes`).
4. CLI surface for new capabilities (`--partial`, `--mixed`).
5. Schema documentation updated (Concat node, decomposed naming, implicit sparse via zeros).

Items originally listed under Phase 2 that were intentionally deferred (moved to later phases for clearer scope slicing):
- Sparse encoding (`sparseFormat: 'csr'`).
- Post-export fusion optimization (collapse decomposed homogeneous layers).
- BatchNormalization / Dropout primitives.
- Residual & skip connections.
- Multiple inputs / outputs.

### Phase 3 (Recurrent / Temporal)
Baseline IMPLEMENTED (extended):
1. Self-recurrence support for ANY hidden layer (one or multiple) in single-step form when `allowRecurrent && recurrentSingleStep`.
  - For each recurrent hidden layer `k` a previous-state input is added:
    - First recurrent layer: `hidden_prev`
    - Subsequent recurrent layers: `hidden_prev_l{k}`
  - Forward path per recurrent layer: Gemm (`gemm_in_l{k}`) + recurrent Gemm (`gemm_rec_l{k}` using `R{k-1}`) -> Add (`add_recurrent_l{k}`) -> Activation (`act_l{k}`).
  - Recurrent weight matrices `Rk` currently diagonal (self-connections only) but sized for future dense intra-layer recurrence.
  - Metadata `recurrent_single_step` now stores JSON array of recurrent layer indices (1-based) instead of boolean.
  - Importer reconstructs self-connections layer-by-layer using each `Rk` diagonal.
2. Tests added covering:
  - Single hidden-layer recurrence (presence of `hidden_prev`, `R0`).
  - Multi-hidden-layer scenario with recurrence only in later layer (presence of `hidden_prev_l2`, `R1` only).
  - Error path for mixed activations in recurrent layer.

Deferred (Phase 3 extended):
- Dense intra-layer recurrence (non-diagonal `Rk`).
- Multi-step unrolling or ONNX `Scan` operator for dynamic sequence length.
- Jordan recurrent (output-to-hidden) connections.
- General backward / arbitrary recurrent edges beyond self; gating interactions.
- Time dimension `[seq, batch, feature]` formalization and unified state carry semantics.

LSTM / GRU Heuristic Sub-plan Progress:
1. Structural Pattern Recognition – COMPLETED (heuristic equal partition + self-connection check; metadata `lstm_groups_stub`).
2. Canonical Parameter Extraction – COMPLETED (concatenated W/R/B initializers with simplified biases).
3. ONNX Node Emission – COMPLETED (emits experimental single-step `LSTM` / `GRU` nodes alongside unfused Gemm path; no pruning yet).
4. Metadata & Fallback – COMPLETED (`lstm_emitted_layers`, `gru_emitted_layers`, `rnn_pattern_fallback`).
5. Import Path Extension – COMPLETED (reconstructs Layer.lstm / Layer.gru using emitted tensors; best-effort, silent skip on mismatch).
6. Testing – IN PROGRESS (to be added now):
  - Unit tests for LSTM/GRU emission presence (initializers + node types) under controlled synthetic layer partitions.
  - Round-trip reconstruction tests verifying gate weight & bias mapping fidelity within tolerance (1e-9) and self-connection restoration.
  - Negative/fallback tests (near-miss sizes recorded as `rnn_pattern_fallback`, incomplete self-connections skip emission).
  - Import robustness tests (missing one of W/R/B causes fused reconstruction skip but base MLP still loads).
7. Deferred (post-step 6):
  - Peephole & projection support.
  - Proper ONNX-compliant bias splitting (Wb/Rb) and gate ordering normalization.
  - Graph pruning/fusion of redundant unfused paths when fused nodes emitted (feature-flagged optimization).
  - Multi-layer stacked fused LSTM/GRU with consistent sequence/time abstraction.

After Step 6 completes, proceed to Phase 4 (Convolutional / Spatial) groundwork (Conv pattern detection spec + minimal Conv export prototype).

### Phase 4 (Convolutional / Spatial)

Status (cumulative so far):
 1. Conv2D Groundwork IMPLEMENTED + TESTS:
   - `conv2dMappings` option + `Conv2DMapping` interface.
   - Exporter emits `Conv` node with `ConvW*` / `ConvB*` initializers; attributes: `kernel_shape`, `strides`, `pads`.
   - Metadata: `conv2d_layers`, `conv2d_specs`.
   - Import reconstructs dense weights (approximate inverse) to preserve existing forward parity.
   - Tests: emission + dimension mismatch fallback (`onnx.conv.groundwork.test.ts`).
 2. Pool2D Mapping IMPLEMENTED + TESTS:
   - `pool2dMappings` option + `Pool2DMapping` interface.
   - Emits `MaxPool` / `AveragePool` nodes directly after target layer output.
   - Metadata: `pool2d_layers`, `pool2d_specs`.
   - Import attaches metadata stub to network (`_onnxPooling`).
   - Tests: pooling metadata presence (`onnx.conv.pool.validation.test.ts`).
 3. Conv Weight Sharing Validation IMPLEMENTED:
   - `validateConvSharing` flag performs best-effort spatial kernel equality check.
   - Metadata: `conv2d_sharing_verified`, `conv2d_sharing_mismatch` (tolerant 1e-9).
   - Tests: sharing success + induced mismatch (`onnx.conv.pool.validation.test.ts`).
 4. Heuristic Conv Inference (non-intrusive) IMPLEMENTED:
   - Detects simple single-channel square input with 2x2 or 3x3 kernel stride 1 producing exact dense size.
   - Metadata only: `conv2d_inferred_layers`, `conv2d_inferred_specs` (does NOT emit Conv node yet).
   - Test: metadata presence (`onnx.conv.infer.test.ts`).
 5. Pooling Import Attachment IMPLEMENTED:
   - Import attaches `_onnxPooling` with layers & specs (no shape simulation yet).
 6. Flatten-after-Pool OPTION IMPLEMENTED:
   - New export option `flattenAfterPooling` inserts `Flatten` node (axis=1) immediately after each emitted Pool.
   - Metadata: `flatten_layers` (export-layer indices where flatten applied).
   - Test: flatten metadata (`onnx.conv.pool.flatten.test.ts`).

New Options / Metadata Added This Increment:
  - `flattenAfterPooling` (export option).
  - Metadata key `flatten_layers` (array of layer indices with flatten bridge inserted).

Deferred / Remaining Phase 4 Scope (updated):
  - Pooling shape simulation & dense remapping on import (currently metadata only; forward path ignores pooling & flatten).
  - Automatic promotion of heuristic `conv2d_inferred_specs` into actual Conv emission behind safety flag.
  - Multi-channel & multi-stage Conv chains (stacked conv/pool pipelines), dilation, groups, depthwise separable conv.
  - Proper spatial shape tracking & validation across Conv → Pool → Flatten boundaries.
  - Activation fusion / redundant Gemm pruning after Conv introduction.
  - Residual / skip spatial connections.
  - Hybrid recurrent + spatial interleaving semantics.

Next Planned Increment (proposed order):
 1. Import-time pooling shape simulation stub: record virtual (H,W,C) after each pool to enable future correctness checks (still no numeric pooling yet; weights unaffected).
 2. Optional dense output adjustment flag: simulate flatten size effect for layers following a flattenAfterPooling site (consistency metadata check only; no weight change yet).
 3. Heuristic Conv auto-promotion (feature-flagged): if sharing validation passes (or small layer), emit Conv instead of Gemm, preserving a fallback path during transition.
 4. Multi-channel heuristic extension: detect repeated kernel tiles across channels, infer inChannels > 1.
 5. Negative tests for flatten + pooling interplay (ensure metadata only – forward outputs unchanged vs baseline).
 6. Begin deeper recurrent parity tests (Phase 3 Step 6) in parallel once spatial metadata foundation stable.

### Phase 5 (Advanced Graph Constructs)
- Attention primitives: map multi-head attention to Q/K/V linear projections + MatMul + Softmax + weighted sum.
- Residual & dense connectivity (DenseNet-like) via Concat + Gemm/Conv + Add nodes.
- Parameter sharing detection (reuse initializer name across multiple consumers).

### Phase 6 (Optimization & Fidelity)
- Operator fusion control (Linear + Bias + Activation fused to single Gemm + Activation or use `Gelu` etc.).
- Constant folding of trivial subgraphs.
- Shape inference tooling (internal) to validate tensor dimension consistency pre-export.

### Phase 7 (Quantization & Precision)
- Export FP16 weights (cast initializers) with model-level precision metadata.
- Post-training static quantization: QLinearMatMul / QLinearConv with scale/zero-point initializers.
- Dynamic quantization (mark nodes for runtime quantization guidance).

### Phase 8 (Custom Ops & Extensibility)
- Custom activation registry: map internal activation to standard (if close) else emit custom domain op `com.neataptic.ActivationX` with attributes referencing a serialized LUT or polynomial approximation.
- Provide `registerOnnxActivation(name, mapperFn)` API allowing external packages to plug in mapping.
- FunctionProto emission for composite custom ops (guarded by opset >= 17 or relevant spec version).

### Phase 9 (Interoperability & Tooling)
- ONNX Runtime test harness (optional dev dependency) to validate exported graphs numerically vs internal forward pass on random inputs.
- CLI: `neataptic export --format onnx --input model.json --out model.onnx` (protobuf) plus `--json` lightweight output.
- Model diff utility: compare two ONNX exports (structure + parameter deltas) for evolutionary lineage inspection.

## 6. Testing Strategy
Layers:
1. Unit tests: layer inference, homogeneity validation, weight/bias serialization integrity, activation mapping.
2. Property-based tests: random MLPs (sizes 1–6 hidden layers) round-trip equality (MSE < 1e-9 for deterministic forward pass).
3. Negative tests: heterogeneous layer activations, missing connections, unsupported recurrent edges → expect descriptive errors.
4. Future: golden ONNX Runtime inference parity set (serialize known networks; validate outputs within tolerance on random batches).
5. Fuzzing (Phase 3+): generate random topologies, attempt export; assert either success or categorized error (no silent corruption).

## 7. Data Structures & API Evolution
Planned interface additions:
```ts
interface OnnxExportOptions {
  opset?: number;               // default 18
  producerName?: string;        // default 'neataptic-ts'
  includeMetadata?: boolean;    // wrap in ModelProto-like object
  batchDimension?: boolean;     // add symbolic batch dim
  allowMixedActivations?: boolean; // relax homogeneity constraint
  legacyNodeOrdering?: boolean; // keep Activation-before-Gemm ordering
  sparseFormat?: 'none' | 'csr';
  quantize?: 'fp16' | 'int8' | false;
}
```
Backward compatibility: default options reproduce current behavior except corrected node order (opt-in legacy).

## 8. Evolutionary Metadata Preservation
- Store innovation IDs & genealogy inside model metadata (`model.graph.doc_string` JSON blob) for traceability.
- Provide a `stripEvolutionMetadata(model)` helper for clean distribution.

## 9. Open Questions / Design TBD
- How to map arbitrary evolved recurrent connections to structured RNN cells without losing semantics? Candidate: limited unrolling + annotation.
- Whether to introduce internal canonicalization pass (topological sort + layering) that becomes the source of truth for both evaluation and export, reducing divergence.
- Policy for unsupported activations: warning vs error vs automatic approximation (e.g., approximate Gaussian with exp-based composite subgraph).

## 10. Risks & Mitigations
- Risk: Export divergence from evaluator leads to incorrect weights (Mitigation: round-trip + ONNX Runtime tests in CI).
- Risk: Explosion of custom ops reduces interoperability (Mitigation: prefer composition of standard ops; gate custom domain usage behind explicit flag).
- Risk: Performance regression from per-neuron decomposition (Mitigation: fusion pass before final emission).

## 11. Short-Term Action Items (Post Phase 2 Completion)
1. Link schema doc from root README & docs index. (Pending)
2. Provide CLI example for exporting an evolved genome (README snippet). (Pending)
3. Property-based randomized topology tests (1–6 hidden layers; varying sparsity & mixed activations) ensure import/export fidelity. (Planned)
4. Fusion optimization pass for decomposed layers (homogeneity collapse). (Planned)
5. Sparse representation design (`sparseFormat` CSR draft spec). (Planned)
6. ONNX Runtime smoke test harness for unified & decomposed models. (Planned)
7. Begin design notes for multi-input/output and residual connection representation (branching graph semantics). (Planned)

## 12. Contribution Guidelines (ONNX Area)
- Keep exporter pure (no side effects except optional console warnings); return new object.
- Add exhaustive test for any new operator mapping (export → import or export → ORT inference).
- Include spec citation (URL + opset version) in code comments when adding new ops.
- Avoid premature optimization; add baseline first then fusion/optimization passes under feature flags.

## 13. Appendix: Future Ideas (Deferred)
- Automatic pruning & weight compression prior to export (magnitude, structured, or evolutionary salience based).
- Multi-objective export scoring (file size, latency) to guide evolution toward deployable architectures.
- Export of training graph (loss node, optimizer state) using ONNX Training extensions (far future).
- Hybrid symbolic + numeric differentiation metadata for advanced downstream tooling.

---

Historical reference: See `network.onnx.ts` for the current implementation baseline. Update this document when phases or APIs land.


Note:

```
Testing requirements:
- all tests should have a single expectation.
- follow AAA pattern (arrange, act, assert) 
- group tests into scenarios with describe(), nest scenarios as needed, no limit on layers.
- when possible, define common testing data directly on the describe() and then write the assertions for it, this also applies for nested scenarios as they each represent more specific cases as it goes down into sub branches.
- aim for 100% testing coverage
- make sure to check existing files before creating/updating one, to be sure you are using the right file, in the right folder, for example `test/neat/` and also to be following the same file pattern inside that folder.

describe(() => {
  describe(() => {
    it('should...');
  });
});

Also:
- Always add JSDocs to all methods, classes, const, let
- Add or update inline comments within methods to explain each step or detail.
- This is an educative NN library, keep the docs detailed and educative
```