# ONNX Export / Import Plan for NeatapticTS

## 0. Purpose
Provide a clear, incremental roadmap from the current minimal MLP ONNX export/import toward broad ONNX compatibility (feed‑forward, recurrent, convolutional, attention, quantized, and extensible custom ops) while preserving NeatapticTS evolutionary features where feasible.

## 1. Current Status (Implemented)
File: `src/architecture/network/network.onnx.ts`

Completed (Phase 0):
- Export (`exportToONNX`) of strictly layered, fully-connected feed‑forward MLPs (inputs → 0+ hidden layers → outputs).
- Import (`importFromONNX`) for models generated by our exporter (round‑trip reproducibility).
- Layer inference & validation: detects non-layered or missing full connectivity; enforces homogeneous activation per non-input layer.
- Supported activations mapped to ONNX: ReLU, Tanh, Sigmoid (Logistic), Identity (fallback with warning for unknown custom activations).
- Parameter export: weights + biases as ONNX initializers (float32 scalars in `float_data`).
- Graph structure assembly with a pair of nodes per layer transition (Gemm + Activation) — currently activation node is emitted before Gemm (historical project ordering, but non-standard for ONNX).
- Bias handling integrated in Gemm (alpha=1, beta=1, transB=1 attributes included).
- Single-layer perceptron edge case handled (inputs → outputs only).

## 2. Gaps vs. Initial Intent
Original plan suggested a per-node mapping (Add / MatMul explicit). Implementation instead uses Gemm (fused MatMul + Add) + Activation pairs — more compact but diverges from the bullet description. Update accepted as design choice; doc corrected accordingly.

## 3. Known Limitations (Current)
- No model metadata (IR version, opset_import, producer_name, doc_string) — output is a lightweight partial ONNX-like JSON, not a spec-complete protobuf structure.
- Node ordering is atypical (Activation before Gemm) — may confuse external tooling performing simple sequential execution assumptions.
- Only float32 dense MLPs; no batching dimension (shape is 1D feature vector, no dynamic axes).
- Homogeneous activation constraint per layer (cannot mix activations inside a layer; output layer forced homogeneous too).
- No support for: sparsity, dropout, batch norm, residual/skip connections, concatenations, branching graphs, shared weights (tying), or partial connectivity.
- Recurrent architectures (Elman/Jordan, LSTM/GRU, arbitrary recurrent links) unsupported.
- Convolutional, pooling, normalization, attention/transformer ops unsupported.
- No quantization, mixed precision, or half/INT8 export.
- Gating & NEAT-specific structural innovations ignored (cannot serialize gating semantics into ONNX yet).
- Custom activations degrade silently to Identity (warning only); no FunctionProto / custom domain registration.
- Import path assumes naming convention `W{idx}`, `B{idx}`, activation node count == layer count; fragile if altered.
- No graph-level validation suite or ONNX Runtime compatibility tests; only internal structural guarantees.

## 4. Design Principles Going Forward
1. Determinism: deterministic ordering of initializers and nodes to allow hashing/comparisons.
2. Spec compliance: progress from a lightweight JSON to either (a) fully spec-compliant JSON matching ONNX ModelProto schema or (b) direct protobuf encoding via an optional dependency.
3. Extensibility: pluggable activation & composite op registry mapping internal functions to ONNX standard ops or custom domains.
4. Graceful degradation: always export a valid subgraph even when certain advanced features need approximation (e.g., replace unsupported activation with Identity + metadata note).
5. Evolution fidelity: preserve NEAT-relevant metadata (innovation numbers, topology tags) in model metadata without breaking external consumers.

## 5. Proposed Phased Roadmap

### Phase 1 (Immediate Hardening)
- Reorder node emission: Gemm → Activation (ONNX conventional ordering). Optionally preserve legacy order via flag `{ legacyOrdering?: boolean }`.
- Add model metadata wrapper: `{ ir_version, opset_import:[{version,domain}], producer_name, producer_version, doc_string }`.
- Add optional batching: allow input shape `[batch, features]` (dynamic first dim). Use symbolic dim for batch (e.g., `dim_param: "N"`).
- Layer-level heterogeneous output support: allow different output activations vs hidden layers.
- Provide round-trip tests (export → import → forward pass numeric equivalence within tolerance) for random seeds.

### Phase 2 (Feature Breadth – Feedforward Enhancements)
- Partial connectivity export: represent missing connections by explicit zeros OR sparse encoding (CSR) + expand to dense for Gemm.
- Support per-node activation (remove homogeneous constraint) by materializing each neuron as: Gemm (slice) + activation OR using `Add`, `Mul`, `Concat` patterns; optimize with fusion pass when all same.
- Add BatchNormalization, Dropout (train-time only metadata, elide at inference), and residual/skip connections (Graph branching).
- Add multiple inputs/outputs (map to multiple graph inputs/outputs).

### Phase 3 (Recurrent / Temporal)
- Simple recurrent (Elman/Jordan) via unrolling for fixed time steps OR using ONNX `Scan` op when sequence length dynamic.
- LSTM/GRU mapping for evolved gated structures (pattern recognition of gate groupings) — potentially constrain evolution to recognizable cell templates for exportability.
- Encode time dimension `[seq, batch, feature]` with symbolic dims.

### Phase 4 (Convolutional / Spatial)
- Detect 2D convolution patterns (weight sharing topologies) and emit Conv + optional Pool + activation.
- Pooling ops: MaxPool, AveragePool; flatten transitions to fully connected.
- Support padding/stride from node metadata (extend internal representation if absent).

### Phase 5 (Advanced Graph Constructs)
- Attention primitives: map multi-head attention to Q/K/V linear projections + MatMul + Softmax + weighted sum.
- Residual & dense connectivity (DenseNet-like) via Concat + Gemm/Conv + Add nodes.
- Parameter sharing detection (reuse initializer name across multiple consumers).

### Phase 6 (Optimization & Fidelity)
- Operator fusion control (Linear + Bias + Activation fused to single Gemm + Activation or use `Gelu` etc.).
- Constant folding of trivial subgraphs.
- Shape inference tooling (internal) to validate tensor dimension consistency pre-export.

### Phase 7 (Quantization & Precision)
- Export FP16 weights (cast initializers) with model-level precision metadata.
- Post-training static quantization: QLinearMatMul / QLinearConv with scale/zero-point initializers.
- Dynamic quantization (mark nodes for runtime quantization guidance).

### Phase 8 (Custom Ops & Extensibility)
- Custom activation registry: map internal activation to standard (if close) else emit custom domain op `com.neataptic.ActivationX` with attributes referencing a serialized LUT or polynomial approximation.
- Provide `registerOnnxActivation(name, mapperFn)` API allowing external packages to plug in mapping.
- FunctionProto emission for composite custom ops (guarded by opset >= 17 or relevant spec version).

### Phase 9 (Interoperability & Tooling)
- ONNX Runtime test harness (optional dev dependency) to validate exported graphs numerically vs internal forward pass on random inputs.
- CLI: `neataptic export --format onnx --input model.json --out model.onnx` (protobuf) plus `--json` lightweight output.
- Model diff utility: compare two ONNX exports (structure + parameter deltas) for evolutionary lineage inspection.

## 6. Testing Strategy
Layers:
1. Unit tests: layer inference, homogeneity validation, weight/bias serialization integrity, activation mapping.
2. Property-based tests: random MLPs (sizes 1–6 hidden layers) round-trip equality (MSE < 1e-9 for deterministic forward pass).
3. Negative tests: heterogeneous layer activations, missing connections, unsupported recurrent edges → expect descriptive errors.
4. Future: golden ONNX Runtime inference parity set (serialize known networks; validate outputs within tolerance on random batches).
5. Fuzzing (Phase 3+): generate random topologies, attempt export; assert either success or categorized error (no silent corruption).

## 7. Data Structures & API Evolution
Planned interface additions:
```ts
interface OnnxExportOptions {
  opset?: number;               // default 18
  producerName?: string;        // default 'neataptic-ts'
  includeMetadata?: boolean;    // wrap in ModelProto-like object
  batchDimension?: boolean;     // add symbolic batch dim
  allowMixedActivations?: boolean; // relax homogeneity constraint
  legacyNodeOrdering?: boolean; // keep Activation-before-Gemm ordering
  sparseFormat?: 'none' | 'csr';
  quantize?: 'fp16' | 'int8' | false;
}
```
Backward compatibility: default options reproduce current behavior except corrected node order (opt-in legacy).

## 8. Evolutionary Metadata Preservation
- Store innovation IDs & genealogy inside model metadata (`model.graph.doc_string` JSON blob) for traceability.
- Provide a `stripEvolutionMetadata(model)` helper for clean distribution.

## 9. Open Questions / Design TBD
- How to map arbitrary evolved recurrent connections to structured RNN cells without losing semantics? Candidate: limited unrolling + annotation.
- Whether to introduce internal canonicalization pass (topological sort + layering) that becomes the source of truth for both evaluation and export, reducing divergence.
- Policy for unsupported activations: warning vs error vs automatic approximation (e.g., approximate Gaussian with exp-based composite subgraph).

## 10. Risks & Mitigations
- Risk: Export divergence from evaluator leads to incorrect weights (Mitigation: round-trip + ONNX Runtime tests in CI).
- Risk: Explosion of custom ops reduces interoperability (Mitigation: prefer composition of standard ops; gate custom domain usage behind explicit flag).
- Risk: Performance regression from per-neuron decomposition (Mitigation: fusion pass before final emission).

## 11. Short-Term Action Items (Ready to Implement)
1. Add metadata wrapper + opset (default 18) & reorder Gemm before activation.
2. Introduce `OnnxExportOptions` with legacy ordering flag.
3. Add unit tests for ordering & round-trip weights (hidden layers up to depth 4).
4. Document current JSON schema vs full ONNX (README section linking here).
5. Provide CLI or script example for exporting a NEAT-evolved best genome.

## 12. Contribution Guidelines (ONNX Area)
- Keep exporter pure (no side effects except optional console warnings); return new object.
- Add exhaustive test for any new operator mapping (export → import or export → ORT inference).
- Include spec citation (URL + opset version) in code comments when adding new ops.
- Avoid premature optimization; add baseline first then fusion/optimization passes under feature flags.

## 13. Appendix: Future Ideas (Deferred)
- Automatic pruning & weight compression prior to export (magnitude, structured, or evolutionary salience based).
- Multi-objective export scoring (file size, latency) to guide evolution toward deployable architectures.
- Export of training graph (loss node, optimizer state) using ONNX Training extensions (far future).
- Hybrid symbolic + numeric differentiation metadata for advanced downstream tooling.

---

Historical reference: See `network.onnx.ts` for the current implementation baseline. Update this document when phases or APIs land.


Note:

```
Testing requirements:
- all tests should have a single expectation.
- follow AAA pattern (arrange, act, assert) 
- group tests into scenarios with describe(), nest scenarios as needed, no limit on layers.
- when possible, define common testing data directly on the describe() and then write the assertions for it, this also applies for nested scenarios as they each represent more specific cases as it goes down into sub branches.
- aim for 100% testing coverage
- make sure to check existing files before creating/updating one, to be sure you are using the right file, in the right folder, for example `test/neat/` and also to be following the same file pattern inside that folder.

describe(() => {
  describe(() => {
    it('should...');
  });
});

Also:
- Always add JSDocs to all methods, classes, const, let
- Add or update inline comments within methods to explain each step or detail.
- This is an educative NN library, keep the docs detailed and educative
```